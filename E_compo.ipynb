{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e08ec61",
   "metadata": {},
   "source": [
    "# (E) COMPO CREATION\n",
    "\n",
    "**REQUIREMENT : (A)**\n",
    "\n",
    "The COMPO Index is only required for the summer process, namely script (F) and (G) if report_type = year.\n",
    "\n",
    "The COMPO index is a standardized composite index which summarise information coming from the BI; SBI and BI2 indexes.\n",
    "\n",
    "COMPO is computed as following for a given date (i):\n",
    "\n",
    "> COMPOi = (BIi standardised + SBIi standardised + BI2i standardised) / 3\n",
    "\n",
    "BIi, SBIi, BI2i are standardised as following (example for BIi):\n",
    "\n",
    "> BIi standardised = (BIi - BI mean) / BI std\n",
    "\n",
    "Means and standard deviations are calculated over a 4-year period (between dteDebut and dteFin, see below).\n",
    "\n",
    "The calculation of COMPO is operated by the function\n",
    "\n",
    "> **compo(sites, dteDebut, dteFin, dteExe)**\n",
    "\n",
    "For each site, this function operated the following steps:\n",
    "\n",
    "**1) Compute_sar_compo_stats_by_sar_id**\n",
    "\n",
    "For each site, calculation of means and standard deviations for the period dteDebut-dteFin.\n",
    "\n",
    "For each site, calculation of standardised BIi, BI2i and SBIi for each available date (i)  over the period dteDebut-dteFin.\n",
    "\n",
    "**2) Fetch_sar_compo_stats_by_sar_id**\n",
    "\n",
    "**3) Compute_compo_time_series_stats**\n",
    "\n",
    "For each site, calculation COMPOi for each available date (i)  over the period dteDebut-dteFin.\n",
    "\n",
    "**4) Compute_smoothed_compo_time_series_stats**\n",
    "\n",
    "For each site, the time series of COMPOi over filled and smoothed using a gaussian filter of 61 days (122 days in total).\n",
    "\n",
    "The gaussian filter is the same as applied in (C).\n",
    "\n",
    "**5) For each site the resulted smoothed COMPO time series is saved is a table**\n",
    "\n",
    "table_name = '{0}_COMPO_{1}_smoothed'.format(sar_id_segment, dteExe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1638ed05-138b-4b6b-8a45-c2b2203e5d06",
   "metadata": {},
   "source": [
    "## Import libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d3271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, datetime\n",
    "sys.path.append(\"/home/gswinnen/SARSAR_Package_RenPri/code/\") # localisation of SARSAR libraries\n",
    "\n",
    "#sys.path.append(\"/home/issep/sarsar-issep/SARSAR_utils/\")                   # emplacement des modules RenPri\n",
    "#sys.path.append(\"/home/issep/sarsar-issep/SARSAR_utils/rme_chg_detection_module/\") # emplacement de la fonction de Mattia\n",
    "\n",
    "from issep import sarsar_admin\n",
    "from os.path import join\n",
    "from lecture_ini import config\n",
    "from csv import reader\n",
    "from select_sites import sites_to_process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650698e5-6247-4e5e-b380-a9ca19bba8a8",
   "metadata": {},
   "source": [
    "## Reading the list of SARs without S2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e29ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading of the list of SAR without S2 data\n",
    "with open(\"/home/gswinnen/Public/liste_sites_sans_NDVI.txt\",'r') as file:\n",
    "    csv_reader = reader(file)\n",
    "    list_lines = list(csv_reader)\n",
    "    list_sites_without_S2_data = []\n",
    "    for item in list_lines:\n",
    "        item = item[0].strip()\n",
    "        if item != '': # check theri is no blank line\n",
    "            if item != \"sar_id_segment\":\n",
    "                list_sites_without_S2_data.append(item)\n",
    "print(list_sites_without_S2_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b616cda6-3d1f-44cd-8c50-047b77f2da52",
   "metadata": {},
   "source": [
    "## Definition of the **compo** function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe8abda-9029-4a04-a653-93aeaba5f7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compo(sites, dteDebut, dteFin, dteExe):\n",
    "    \"\"\" This function computes the COMPO composite raw and smoothed index values over a given period.\n",
    "        Results are registered in new tables in the DB.\n",
    "            \n",
    "            Parameters (they are automatically read in \"sarsar.ini\" by the function \"config\"):\n",
    "                sites (list): list of sites (sar_id_segment) to process\n",
    "                dteDebut (date): date from which we need data (YYYY-MM-DD)\n",
    "                dteFin (date): date until which we need data (YYYY-MM-DD)\n",
    "                dteExe: processing/execution date (YYYYMMDD)\n",
    "    \"\"\"\n",
    "\n",
    "    ## CONNECTION TO DB\n",
    "    # Define Database connection parameters\n",
    "    # NOTE: password is in ~/.pgpass\n",
    "    credentials = config(section='postgresql')\n",
    "\n",
    "    db_credentials = {\n",
    "        'host': credentials['host'],\n",
    "        'user': credentials['user'],\n",
    "        'db' : credentials['database']\n",
    "    }\n",
    "\n",
    "    # ALWAYS prepare env et the beginning\n",
    "    print('> Preparing env (DB credentials, etc)')\n",
    "    sarsar_admin.prepare_env(db_credentials)\n",
    "    \n",
    "    conn = sarsar_admin._create_or_get_db_connection()\n",
    "    cur = None\n",
    "    cur2 = None\n",
    "    \n",
    "    ## STARTING COMPO CALCULATION\n",
    "    \n",
    "    try:\n",
    "        import psycopg2.extras\n",
    "        cur = conn.cursor(cursor_factory = psycopg2.extras.DictCursor)\n",
    "        cur2 = conn.cursor(cursor_factory = psycopg2.extras.DictCursor)\n",
    "        \n",
    "#            strSQL = '''SELECT sar_id_segment, index_name, acq_date, index_mean, pixel_count \n",
    "#                        FROM sar_index_stats WHERE sar_id_segment = '{0}' AND index_name NOT IN ('BI2','VV','SBI','NDVI','BI2_part1','BI','BAI','VH') \n",
    "#                        AND substring(index_name,1,2) != 'VV' AND acq_date BETWEEN '{1}' AND '{2}' \n",
    "#                        ORDER BY sar_id_segment, index_name, acq_date;'''.format(site, dteDebut, dteFin)\n",
    "        \n",
    "        compteur = 0\n",
    "#        cur.execute('SELECT DISTINCT sar_id_segment FROM sar_index_stats;')\n",
    "#        result = cur.fetchall()    \n",
    "        \n",
    "        #result = ['52063-ISA-0020-01'] #DEBUG\n",
    "        \n",
    "        for site in sites:\n",
    "            #sar_id_segment = dict(site)['sar_id_segment'] #Un-comment once debugged\n",
    "            sar_id_segment = site\n",
    "            #sar_id_segment = '52063-ISA-0020-01'\n",
    "#            sar_id_segment = '56078-ISA-0008-01'  # DEBUG compute_sar_compo_stats_by_sar_id error\n",
    "#            sar_id_segment = '25072-ISA-0013-01'  # DEBUG\n",
    "            compteur += 1\n",
    "            print(\"> \",compteur,sar_id_segment)\n",
    "        # Checking that the current site has S2 data\n",
    "            if sar_id_segment not in list_sites_without_S2_data:\n",
    "                # Checking the data availability before calling the sarsar_admin.compute_sar_compo_stats_by_sar_id function;\n",
    "                # because it calls a stored procedure of the PostgreSQL DB which raises an error when there is no \n",
    "                # data in the sar_index_stats table for the requested site, indices and date range...\n",
    "                strSQL = '''SELECT count(*) FROM sar_index_stats\n",
    "                            WHERE sar_id_segment = '{0}' AND acq_date BETWEEN '{1}' AND '{2}'\n",
    "                            AND index_name IN ('BI', 'BI2', 'SBI');'''.format(sar_id_segment, dteDebut, dteFin)\n",
    "                #print(strSQL)\n",
    "                cur2.execute(strSQL)\n",
    "                computables = cur2.fetchone()\n",
    "                \n",
    "                # If not enough data, results is useless (and std can't be calculated)\n",
    "                if computables[0] > 10:\n",
    "                    \n",
    "                    # Computing and fetching the index components for the site and date range.\"\n",
    "                    # Removing the dashes from the dates, to suit the compo_stats function, )\n",
    "                    sarsar_admin.compute_sar_compo_stats_by_sar_id(sar_id_segment, dteDebut.replace('-', ''), dteFin.replace('-', ''))\n",
    "                    compo_stats = sarsar_admin.fetch_sar_compo_stats_by_sar_id(sar_id_segment, dteDebut.replace('-', ''), dteFin.replace('-', '') )\n",
    "                else:\n",
    "                    compo_stats = []\n",
    "\n",
    "#                print('The are %i records (acquisition dates with COMPO values) for this specific COMPO serie (sar_id_segment AND start_date_incl AND end_date_incl)' % len(compo_stats))\n",
    "\n",
    "\n",
    "                if len(compo_stats) > 0:\n",
    "        \n",
    "                    # Computing COMPO raw values\n",
    "                    compo_ts = sarsar_admin.compute_compo_time_series_stats(compo_stats)\n",
    "\n",
    "                    # Smoothing COMPO raw value time serie\n",
    "                    smoothed_compo_ts = sarsar_admin.compute_smoothed_compo_time_series_stats(compo_stats, time_resampling='D', gaussian_sigma=61)\n",
    "\n",
    "                    # NB: smoothed_compo_ts is a tuple of 2 arrays: first is an array of interpolated dates, second is an array of corresponding smoothed stats\n",
    "                    smoothed_profile_dico = {}\n",
    "                    \n",
    "    ## SAVING OUTPUTS IN TABLES\n",
    "    \n",
    "                    #DEBUG : for i in range(len(compo_stats)):\n",
    "                    #DEBUG : print(len(compo_stats),len(smoothed_compo_ts[0]))\n",
    "                    for i in range(len(smoothed_compo_ts[0])):\n",
    "                        # transformation of smoothed_compo_ts into a dictionnary\n",
    "                        smoothed_profile_dico[smoothed_compo_ts[0][i].strftime(\"%Y-%m-%d\")] = smoothed_compo_ts[1][i]\n",
    "\n",
    "                    # DEBUG : pause = input(\"pressez une touche...\")\n",
    "\n",
    "                    # Saving COMPO smoothed time serie in a table named \"{sar_id_segment}_COMPO_{dteExe}_smoothed\"\n",
    "                    # Defining result table name\n",
    "                    table_name = '{0}_COMPO_{1}_smoothed'.format(sar_id_segment, dteExe)\n",
    "\n",
    "#                    print(table_name)  # DEBUG\n",
    "                    # Checking table existence\n",
    "                    strSQL = 'DROP TABLE IF EXISTS \"{0}\";'.format(table_name)\n",
    "                    cur.execute(strSQL)\n",
    "                    conn.commit()\n",
    "                    # Table creation\n",
    "                    strSQL = 'CREATE TABLE IF NOT EXISTS \"{0}\" (dte DATE, indice NUMERIC);'.format(table_name)  #, ', '.join(type_valeurs))\n",
    "                    cur.execute(strSQL)\n",
    "\n",
    "                    # Filling table with results\n",
    "                    for item in smoothed_profile_dico:\n",
    "                        strSQL = 'INSERT INTO \"{0}\" (dte, indice) VALUES (\\'{1}\\', {2});'.format(table_name, item, smoothed_profile_dico[item])  # , (', '.join(champs)), str(tuple(valeurs)))\n",
    "                        cur.execute(strSQL)\n",
    "\n",
    "                    # Posting allmodifications\n",
    "                    conn.commit()\n",
    "                else:\n",
    "                    # Error message if not enough data to compute COMPO\n",
    "                    print(\"COMPO can't be computed : not enough data\")\n",
    "                \n",
    "            else:\n",
    "                # Error message if a given site has no S2 data\n",
    "                print(\"segment ignored\")\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "        print('sar_id_segment =', sar_id_segment)\n",
    "    finally:\n",
    "        if cur is not None:\n",
    "            cur.close()\n",
    "            \n",
    "    # ALWAYS release env at the end\n",
    "    print('> Releasing env')\n",
    "    sarsar_admin.release_env()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919dc974-c4bf-4524-89b8-49195e81ff2e",
   "metadata": {},
   "source": [
    "## Jupyter notebook conversion in python script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b87313-eb37-4e00-a7a3-051be1ff4327",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script E_compo.ipynb\n",
    "\n",
    "with open('E_compo.py', 'r', errors='ignore') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "with open('E_compo.py', 'w') as f:\n",
    "    for line in lines:\n",
    "        if 'nbconvert --to script' in line:\n",
    "            break\n",
    "        else:\n",
    "            if '# In[' not in line[:10]:\n",
    "                f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7dbc06-3711-43ec-81aa-e16a0f4a3cb9",
   "metadata": {},
   "source": [
    "## Calling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7611861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call config\n",
    "dates = config(section='dates')\n",
    "dteDebut = dates['deb']\n",
    "dteFin = dates['fin']\n",
    "dteExe = dates['exe']\n",
    "\n",
    "## Call sites_to_process\n",
    "lstSARs = sites_to_process(dteDebut, dteFin)\n",
    "\n",
    "# Call compo\n",
    "compo(lstSARs, dteDebut, dteFin, dteExe)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9caec29d",
   "metadata": {},
   "source": [
    "La fonction python ne fait qu'enclancher une procédure stockée dans la DB ; qui lève une erreur...\n",
    "=> Cherche la définition de cette procédure stockée pour voir ce qui génère l'erreur :\n",
    "\n",
    "select p.proname, p.prosrc from pg_proc as p where p.proname='compute_sar_compo_stats_by_sar_id';\n",
    "\n",
    "Là, je vois que c'est cette ligne qui lève une erreur lorsqu'il n'y a pas de données dans la table sar_index_stats pour le site, les indices et l'intervalle de dates demandés :\n",
    "select count(sis.*) into temp_int from sar_index_stats sis where sis.sar_id_segment = $1\n",
    " and sis.acq_date >= TO_DATE($2, 'YYYYMMDD') and sis.acq_date <= TO_DATE($3, 'YYYYMMDD') and sis.index_name in ('BI', 'BI2', 'SBI');\n",
    " \n",
    "=> la solution est de vérifier la disponibilité des données avant d'appeler la fonction :\n",
    "\n",
    "strSQL = '''SELECT count(*) from sar_index_stats\n",
    "            WHERE sar_id_segment = {0} AND acq_date BETWEEN {1} AND {2}\n",
    "            AND index_name IN ('BI', 'BI2', 'SBI');'''.format()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc319a70",
   "metadata": {},
   "source": [
    "null value in column \"bi_ts_mean\" violates not-null constraint\n",
    "DETAIL:  Failing row contains (62063-ISA-0008-01, 2022-03-03 10:41:45.709003, 2017-12-01, 2022-01-31, 0, 142, null, null, null, null, null, null).\n",
    "CONTEXT:  SQL statement \"insert into sar_compo_inputs (\n",
    "\t\t\tsar_id_segment,\n",
    "\t\t\tcomp_time,\n",
    "\t\t\tstart_date_incl,\n",
    "\t\t\tend_date_incl,\n",
    "\t\t\tnb_dates_wo_outliers,\n",
    "\t\t\tnb_filtered_outliers,\n",
    "\t\t\tbi_ts_mean,\n",
    "\t\t\tbi_ts_stddev,\n",
    "\t\t\tbi2_ts_mean,\n",
    "\t\t\tbi2_ts_stddev,\n",
    "\t\t\tsbi_ts_mean,\n",
    "\t\t\tsbi_ts_stddev\n",
    "\t\t) values (\n",
    "\t\t\t$1,\n",
    "\t\t\ttime_keep,\n",
    "\t\t\tTO_DATE($2, 'YYYYMMDD'),\n",
    "\t\t\tTO_DATE($3, 'YYYYMMDD'),\n",
    "\t\t\tnb_dates_wo_outliers,\n",
    "\t\t\tnb_filtered_outliers,\n",
    "\t\t\tbi_ts_mean,\n",
    "\t\t\tbi_ts_stddev,\n",
    "\t\t\tbi2_ts_mean,\n",
    "\t\t\tbi2_ts_stddev,\n",
    "\t\t\tsbi_ts_mean,\n",
    "\t\t\tsbi_ts_stddev\n",
    "\t\t)\"\n",
    "PL/pgSQL function compute_sar_compo_stats_by_sar_id(character varying,character varying,character varying) line 93 at SQL statement\n",
    "\n",
    "current transaction is aborted, commands ignored until end of transaction block\n",
    "\n",
    "sar_id_segment = 62063-ISA-0008-01\n",
    "\n",
    "\n",
    "=> vérifie si on a des données S2 pour ce site :\n",
    "select * from sar_index_stats where sar_id_segment = '62063-ISA-0008-01' and index_name = 'SBI' and acq_date > '2021-05-01';\n",
    "\n",
    "On a bien des données => soumettre à Renato la liste des sites 'à problèmes'...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f656ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ada75f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9475de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_time_serie(dteDebut, dteFin, dteExe):\n",
    "    \n",
    "    # List all indices\n",
    "    indices = ['BAI','BI','BI2','NDVI','NDWI2','SBI']\n",
    "        \n",
    "    # Define Database connection parameters\n",
    "    # NOTE: password is in ~/.pgpass\n",
    "    credentials = config(section='postgresql')\n",
    "\n",
    "    db_credentials = {\n",
    "        'host': credentials['host'],\n",
    "        'user': credentials['user'],\n",
    "        'db' : credentials['database']\n",
    "    }\n",
    "\n",
    "    # ALWAYS prepare env et the beginning\n",
    "    print('> Preparing env (DB credentials, etc)')\n",
    "    sarsar_admin.prepare_env(db_credentials)\n",
    "    \n",
    "    # Process all sar_id_segments\n",
    "    conn = sarsar_admin._create_or_get_db_connection()\n",
    "    cur = None\n",
    "\n",
    "    try:\n",
    "        import psycopg2.extras\n",
    "        cur = conn.cursor(cursor_factory = psycopg2.extras.DictCursor)\n",
    "        cur.execute('SELECT DISTINCT sar_id_segment FROM sar_index_stats;')    \n",
    "        result = cur.fetchall()\n",
    "        for row in result:\n",
    "            sar_id_segment = dict(row)\n",
    "            \n",
    "            \n",
    "            for indice in indices :\n",
    "                print('> Processing smoothing for %s, %s' % (sar_id_segment, indice))\n",
    "                raw_profile = sarsar_admin.fetch_sar_index_stats_by_sar_id(sar_id_segment['sar_id_segment'], indice, dteDebut, dteFin )\n",
    "                smoothed_profile = sarsar_admin.compute_smoothed_time_series_stats(raw_profile)  # .remove_outliers_from_stats()\n",
    "\n",
    "                # transpose les deux arrays dans un dictionnaire tel qu'attendu pour la boucle suivante...\n",
    "                smoothed_dates = smoothed_profile[0]\n",
    "                smoothed_values = smoothed_profile[1]\n",
    "                smoothed_profile_dico = {}\n",
    "\n",
    "                for i in range(len(smoothed_dates)):\n",
    "                    smoothed_profile_dico[smoothed_dates[i].strftime(\"%Y-%m-%d\")] = smoothed_values[i]\n",
    "\n",
    "                # (re)crée LA table \"{sar_id_segment}_{indice}_{dteExe}_smoothed\" qui doit accueillir ces données\n",
    "                table_name = '{0}_{1}_{2}_smoothed'.format(sar_id_segment['sar_id_segment'], indice, dteExe)\n",
    "                strSQL = 'DROP TABLE IF EXISTS \"{0}\";'.format(table_name)  # Encadrer le nom de table pour tolérer les chiffres en début et les tirets au milieu\n",
    "                cur.execute(strSQL)\n",
    "                conn.commit()\n",
    "\n",
    "                strSQL = 'CREATE TABLE IF NOT EXISTS \"{0}\" (dte DATE, indice NUMERIC);'.format(table_name)  #, ', '.join(type_valeurs))\n",
    "                cur.execute(strSQL)\n",
    "\n",
    "                for item in smoothed_profile_dico:\n",
    "                    strSQL = 'INSERT INTO \"{0}\" (dte, indice) VALUES (\\'{1}\\', {2});'.format(table_name, item, smoothed_profile_dico[item])  # , (', '.join(champs)), str(tuple(valeurs)))\n",
    "                    cur.execute(strSQL)\n",
    "\n",
    "            # Poste toutes les modifications\n",
    "            conn.commit()\n",
    "            \n",
    "        cur.close()\n",
    "        \n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "    finally:\n",
    "        if cur is not None:\n",
    "            cur.close()\n",
    "            \n",
    "    # ALWAYS release env at the end\n",
    "    print('> Releasing env')\n",
    "    sarsar_admin.release_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7864d3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG: appelle la fonction\n",
    "\n",
    "dates = config(section='dates')\n",
    "dteDebut = dates['deb']\n",
    "dteFin = dates['fin']\n",
    "dteExe = dates['exe']\n",
    "\n",
    "smooth_time_serie(dteDebut, dteFin, dteExe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5cd480",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
